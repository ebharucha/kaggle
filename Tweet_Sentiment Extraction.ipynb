{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_Sentiment.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "8Yc-ZYJxOoGm",
        "6y8TzgThTxcr",
        "4lhnYjnH8HAX",
        "NZL1OSlVD7b0",
        "v0X1S-BQD3wQ",
        "Cy5eUDK1Xivq",
        "BuMyAQuiuTfB",
        "vKiAaiuJsB6E",
        "vOI-oFTHr3bt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DDCN-jBLNvmr"
      },
      "source": [
        "# [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FKPnioFaQGLs"
      },
      "source": [
        "## Mount Google Drive\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "55HGs2NAQIwi",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv_4uRCWH2i4",
        "colab_type": "text"
      },
      "source": [
        "## Temporary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh5JqN69H5RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tokenizers >> /dev/null\n",
        "!pip install transformers >> /dev/null\n",
        "!pip install bert-for-tf2 >> /dev/null\n",
        "# !pip install sentencepiece >> /dev/null\n",
        "# !python -m spacy download en_core_web_lg >> /dev/null\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "DATADIR = '/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data'\n",
        "import sys\n",
        "sys.path.insert(0, f\"{DATADIR}/models/sentencepiece/\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "import spacy\n",
        "import random\n",
        "import plac\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "import en_core_web_sm\n",
        "# import en_core_web_lg\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "from transformers import BertTokenizer, AlbertTokenizer, TFBertModel, TFRobertaModel, RobertaConfig, XLMRobertaConfig, TFXLMRobertaModel\n",
        "# import sentencepiece as spm\n",
        "# import sentencepiece_pb2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "# from bert.tokenization.bert_tokenization import FullTokenizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df_train = pd.read_csv(f'{DATADIR}/train.csv').dropna().reset_index(drop=True)\n",
        "df_test = pd.read_csv(f'{DATADIR}/test.csv').dropna().reset_index(drop=True)\n",
        "df_train[df_train.isna().any(axis=1)], df_test[df_test.isna().any(axis=1)]\n",
        "df_train.shape, df_test.shape\n",
        "\n",
        "# How many tweets where there is no overlap between text & selected_text - basically bad labels\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "jaccard_text_sel_text = []\n",
        "for _, row in df_train.iterrows():\n",
        "    jaccard_text_sel_text.append(jaccard(row.text, row.selected_text))\n",
        "        \n",
        "df_train['jaccard_text_sel_text'] = jaccard_text_sel_text\n",
        "bad_labels_total = df_train[df_train.jaccard_text_sel_text == 0].shape[0]\n",
        "bad_labels_positive = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'positive')].shape[0]\n",
        "bad_labels_negative = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'negative')].shape[0]\n",
        "bad_labels_neutral = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'neutral')].shape[0]\n",
        "\n",
        "print (f'Total # of training samples = {df_train.shape[0]:,}')\n",
        "\n",
        "print (f'Total number of bad labels = {bad_labels_total}')\n",
        "print (f'Total number of positive bad labels = {bad_labels_positive:,}')\n",
        "print (f'Total number of negative bad labels = {bad_labels_negative:,}')\n",
        "print (f'Total number of negative neutral labels = {bad_labels_neutral:,}')\n",
        "\n",
        "# Remove rows with bad labels\n",
        "df_train = df_train[df_train.jaccard_text_sel_text != 0].reset_index(drop=True)\n",
        "df_train.drop(columns=['jaccard_text_sel_text'], inplace=True)\n",
        "\n",
        "print (f'Remaining # of training samples post removal of bad label rows = {df_train.shape[0]:,}')\n",
        "\n",
        "\n",
        "# Basic text cleanup\n",
        "\n",
        "clean = lambda s: s.lower().strip()\n",
        "\n",
        "def clean_text(s):\n",
        "    s = s.lower()     # Convert to lowercase\n",
        "    # s = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))', '', s)     # Remove URLs\n",
        "#     s = re.sub(r'\\d+', '', s)     # Remove numbers\n",
        "#     s = s.translate(str.maketrans('', '', string.punctuation))     # Remove punctuation\n",
        "    s = s.strip()     # Remove leading & trailing whitespaces\n",
        "    s = re.sub('[ ]{2,}', ' ', s)   # replace multiple white spaces within tweet with single white space\n",
        "    # s = re.sub(r'https?', '', s)      #Remove remnants of http | https\n",
        "#     s = s.encode(\"ascii\", \"ignore\").decode()     # Remove non-ascii characters\n",
        "    return (s)\n",
        "\n",
        "\n",
        "def process_df(df, train):\n",
        "    text_clean = []\n",
        "    sel_text_clean = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if (row.sentiment != 'neutral'):\n",
        "            if (train == True):\n",
        "                text_clean.append(clean_text(row.text))\n",
        "                sel_text_clean.append(clean_text(row.selected_text))\n",
        "            else:\n",
        "                text_clean.append(clean_text(row.text))\n",
        "        else:\n",
        "            if (train == True):\n",
        "                text_clean.append(clean(row.text))\n",
        "                sel_text_clean.append(clean(row.selected_text))\n",
        "            else:\n",
        "                text_clean.append(clean(row.text))\n",
        "    if (train == True):\n",
        "        return (text_clean, sel_text_clean)\n",
        "    else:\n",
        "        return(text_clean)\n",
        "            \n",
        "(text_clean, sel_text_clean) = process_df(df_train, True)\n",
        "df_train['text_clean'] = text_clean\n",
        "df_train['sel_text_clean'] = sel_text_clean\n",
        "\n",
        "(text_clean) = process_df(df_test, False)\n",
        "df_test['text_clean'] = text_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soshr_nswYJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "8Yc-ZYJxOoGm"
      },
      "source": [
        "## Import dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "27COAZyPOMyy",
        "colab": {}
      },
      "source": [
        "#to access kaggle datasets\n",
        "!pip install kaggle\n",
        "\n",
        "# Colab's file access feature\n",
        "from google.colab import files\n",
        "\n",
        "#retrieve uploaded file\n",
        "uploaded = files.upload()\n",
        "\n",
        "#print results\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !unzip train.csv.zip -d '/content/gdrive/My Drive/Colab Notebooks/data1/Tweet_Sentiment/'\n",
        "#!mv *.csv '/content/gdrive/My Drive/Colab Notebooks/data1/Tweet_Sentiment/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aJk6DvHaUikp"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqAZsUDPUlp9",
        "colab": {}
      },
      "source": [
        "!pip install tokenizers >> /dev/null\n",
        "!pip install transformers >> /dev/null\n",
        "!pip install bert-for-tf2 >> /dev/null\n",
        "# !pip install sentencepiece >> /dev/null\n",
        "# !python -m spacy download en_core_web_lg >> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hNPMhq0OTFPR"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3mNYLLvWTH54",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "DATADIR = '/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data'\n",
        "import sys\n",
        "sys.path.insert(0, f\"{DATADIR}/models/sentencepiece/\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "import spacy\n",
        "import random\n",
        "import plac\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "import en_core_web_sm\n",
        "# import en_core_web_lg\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "from transformers import BertTokenizer, AlbertTokenizer, TFBertModel, TFRobertaModel, RobertaConfig, XLMRobertaConfig, TFXLMRobertaModel\n",
        "# import sentencepiece as spm\n",
        "# import sentencepiece_pb2\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "# from bert.tokenization.bert_tokenization import FullTokenizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "VMIsOVSzSy6p"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Dhr6Pd1zNedv",
        "colab": {}
      },
      "source": [
        "DATADIR = '/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data'\n",
        "\n",
        "df_train = pd.read_csv(f'{DATADIR}/train.csv').dropna().reset_index(drop=True)\n",
        "df_test = pd.read_csv(f'{DATADIR}/test.csv').dropna().reset_index(drop=True)\n",
        "df_train[df_train.isna().any(axis=1)], df_test[df_test.isna().any(axis=1)]\n",
        "df_train.shape, df_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2fUNoRl1ikd",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "6cyGrD1gSAoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many tweets where there is no overlap between text & selected_text - basically bad labels\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "jaccard_text_sel_text = []\n",
        "for _, row in df_train.iterrows():\n",
        "    jaccard_text_sel_text.append(jaccard(row.text, row.selected_text))\n",
        "        \n",
        "df_train['jaccard_text_sel_text'] = jaccard_text_sel_text\n",
        "bad_labels_total = df_train[df_train.jaccard_text_sel_text == 0].shape[0]\n",
        "bad_labels_positive = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'positive')].shape[0]\n",
        "bad_labels_negative = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'negative')].shape[0]\n",
        "bad_labels_neutral = df_train[(df_train.jaccard_text_sel_text == 0) & (df_train.sentiment == 'neutral')].shape[0]\n",
        "\n",
        "print (f'Total # of training samples = {df_train.shape[0]:,}')\n",
        "\n",
        "print (f'Total number of bad labels = {bad_labels_total}')\n",
        "print (f'Total number of positive bad labels = {bad_labels_positive:,}')\n",
        "print (f'Total number of negative bad labels = {bad_labels_negative:,}')\n",
        "print (f'Total number of negative neutral labels = {bad_labels_neutral:,}')\n",
        "\n",
        "# Remove rows with bad labels\n",
        "df_train = df_train[df_train.jaccard_text_sel_text != 0].reset_index(drop=True)\n",
        "df_train.drop(columns=['jaccard_text_sel_text'], inplace=True)\n",
        "\n",
        "print (f'Remaining # of training samples post removal of bad label rows = {df_train.shape[0]:,}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Lem7VQtIBYvs",
        "colab": {}
      },
      "source": [
        "# Check how many training sampels are provided for different sentiments\n",
        "# Looks to be a fairly balanced training dataset\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Train')\n",
        "sns.countplot(x='sentiment', data=df_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Test')\n",
        "sns.countplot(x='sentiment', data=df_test)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "crahlXCGSAos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check how the \"text\" vs. \"selected_text\" varies for different sentiments\n",
        "# It appears that for a majority of \"neutral\" tweets the text & selected_text are the same.\n",
        "\n",
        "char_count = lambda s: len(s.strip())\n",
        "df_train['text_len'] = df_train.text.apply(char_count)\n",
        "df_train['sel_text_len'] = df_train.selected_text.apply(char_count)\n",
        "df_train['delta_text_sel_text_len'] = df_train.text_len - df_train.sel_text_len\n",
        "\n",
        "# How many neutral tweets have the same text vs. selected_text \n",
        "a = df_train[df_train.sentiment == 'neutral'].shape[0]\n",
        "b = df_train[(df_train.delta_text_sel_text_len == 0) & (df_train.sentiment == 'neutral')].shape[0]\n",
        "print (f'{b:,} out of {a:,} ({b/a*100:.2f}%) neutral sentiment tweets are the same')\n",
        "\n",
        "# How many positive tweets have the same text vs. selected_text \n",
        "a = df_train[df_train.sentiment == 'positive'].shape[0]\n",
        "b = df_train[(df_train.delta_text_sel_text_len == 0) & (df_train.sentiment == 'positive')].shape[0]\n",
        "print (f'{b:,} out of {a:,} ({b/a*100:.2f}%) positive sentiment tweets are the same')\n",
        "\n",
        "# How many negative tweets have the same text vs. selected_text \n",
        "a = df_train[df_train.sentiment == 'negative'].shape[0]\n",
        "b = df_train[(df_train.delta_text_sel_text_len == 0) & (df_train.sentiment == 'negative')].shape[0]\n",
        "print (f'{b:,} out of {a:,} ({b/a*100:.2f}%) negative sentiment tweets are the same')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "PgLI7mrQSAou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count words in text, selected text & compute difference\n",
        "\n",
        "c = lambda s: len(s.strip().split())\n",
        "\n",
        "df_train['text_wc'] = df_train['text'].apply(c)\n",
        "df_train['seltext_wc'] = df_train['selected_text'].apply(c)\n",
        "df_train['delta_text_seltext_wc'] = df_train['text_wc'] - df_train['seltext_wc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "FgFYhFE_FBTQ",
        "colab": {}
      },
      "source": [
        "# Check how the \"text\" vs. \"selected_text\" varies for non-neutral tweets with 2 to 5 words\n",
        "\n",
        "y0 = 0\n",
        "y1 = 0\n",
        "plt.figure(figsize=(15,10))\n",
        "for i in range (2,6):\n",
        "  df_train_shortTweet = df_train[(df_train.text_wc <= i) & (df_train.sentiment != \"neutral\")]\n",
        "  plt.subplot(2,2,i-1)\n",
        "  plt.title(f'delta_text_seltext_wc for {i} words')\n",
        "  sns.countplot(x='delta_text_seltext_wc', data=df_train_shortTweet)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "GaxnnWEySAoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check how the \"text\" vs. \"selected_text\" varies for non-neutral sentiments containing URLs\n",
        "# There appear to be very few non-neutral selected text entries with URLs\n",
        "\n",
        "f = lambda s: r'//' in s\n",
        "\n",
        "df_train['URLs_text'] = df_train.text.apply(f)\n",
        "df_train['URLs_sel_text'] = df_train.selected_text.apply(f)\n",
        "\n",
        "print (f'Number of non neutral sentiment text with URLs = {df_train[(df_train.URLs_text == True) & (df_train.sentiment != \"neutral\")].shape[0]}')\n",
        "print (f'Number of non neutral sentiment selected_text with URLs = {df_train[(df_train.URLs_sel_text == True) & (df_train.sentiment != \"neutral\")].shape[0]}')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.countplot(x = 'URLs_text', data = df_train)\n",
        "plt.subplot(1,2,2)\n",
        "sns.countplot(x = 'URLs_sel_text', data = df_train)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "7EhzIKIySAo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for non-ascii\n",
        "# There appear to be very few non-neutral selected text entries with non-ascii charaters\n",
        "\n",
        "f = lambda x: all(ord(char) < 128 for char in x)\n",
        "df_train[(df_train.selected_text.apply(f) == False)  & (df_train.sentiment != 'neutral')]\n",
        "\n",
        "\n",
        "print (f'Number of non neutral sentiment tweets with non-ascii chars = {df_train[(df_train.selected_text.apply(f) == False) & (df_train.sentiment != \"neutral\")].shape[0]}')\n",
        "print (f'Number of non neutral sentiment selected_text with non-ascii chars = {df_train[(df_train.selected_text.apply(f) == False) & (df_train.sentiment != \"neutral\")].shape[0]}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fqn7CS8nSAo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for numeric characters in tweets\n",
        "\n",
        "df_train[(df_train.selected_text.str.contains(r'\\d+', regex=True) == True) & (df_train.sentiment != 'neutral')].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "S39lUsQ_SAo4",
        "colab_type": "text"
      },
      "source": [
        "## Basic data cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "uL7XzRrqSAo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic text cleanup\n",
        "\n",
        "clean = lambda s: s.lower().strip()\n",
        "\n",
        "def clean_text(s):\n",
        "    s = s.lower()     # Convert to lowercase\n",
        "    # s = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))', '', s)     # Remove URLs\n",
        "#     s = re.sub(r'\\d+', '', s)     # Remove numbers\n",
        "#     s = s.translate(str.maketrans('', '', string.punctuation))     # Remove punctuation\n",
        "    s = s.strip()     # Remove leading & trailing whitespaces\n",
        "    s = re.sub('[ ]{2,}', ' ', s, 10)   # replace multiple white spaces within tweet with single white space\n",
        "    # s = re.sub(r'https?', '', s)      #Remove remnants of http | https\n",
        "#     s = s.encode(\"ascii\", \"ignore\").decode()     # Remove non-ascii characters\n",
        "    return (s)\n",
        "\n",
        "\n",
        "def process_df(df, train):\n",
        "    text_clean = []\n",
        "    sel_text_clean = []\n",
        "    for idx, row in df.iterrows():\n",
        "        if (row.sentiment != 'neutral'):\n",
        "            if (train == True):\n",
        "                text_clean.append(clean_text(row.text))\n",
        "                sel_text_clean.append(clean_text(row.selected_text))\n",
        "            else:\n",
        "                text_clean.append(clean_text(row.text))\n",
        "        else:\n",
        "            if (train == True):\n",
        "                text_clean.append(clean(row.text))\n",
        "                sel_text_clean.append(clean(row.selected_text))\n",
        "            else:\n",
        "                text_clean.append(clean(row.text))\n",
        "    if (train == True):\n",
        "        return (text_clean, sel_text_clean)\n",
        "    else:\n",
        "        return(text_clean)\n",
        "            \n",
        "(text_clean, sel_text_clean) = process_df(df_train, True)\n",
        "df_train['text_clean'] = text_clean\n",
        "df_train['sel_text_clean'] = sel_text_clean\n",
        "\n",
        "(text_clean) = process_df(df_test, False)\n",
        "df_test['text_clean'] = text_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpif13YhSAo6",
        "colab_type": "text"
      },
      "source": [
        "## Pickle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdV5K-GISAo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open (f'{DATADIR}/df_train_pos_neg_good.pkl', 'wb') as pklfile:\n",
        "#   pickle.dump(df_train_pos_neg_good, pklfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH0uu3eTSAo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATADIR = '/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data'\n",
        "\n",
        "# Pickle data\n",
        "\n",
        "# with open (f'{DATADIR}/df_train.pkl', 'wb') as pklfile:\n",
        "#   pickle.dump(df_train, pklfile)\n",
        "\n",
        "# with open (f'{DATADIR}/df_test.pkl', 'wb') as pklfile:\n",
        "#   pickle.dump(df_test, pklfile)\n",
        "\n",
        "\n",
        "with open (f'{DATADIR}/df_train.pkl', 'rb') as pklfile:\n",
        "  df_train = pickle.load(pklfile)\n",
        "\n",
        "with open (f'{DATADIR}/df_test.pkl', 'rb') as pklfile:\n",
        "  df_test = pickle.load(pklfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONgnFs6rSAo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LDZmNUfyx4v1"
      },
      "source": [
        "## Develop Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "jGgvhahYX8j-"
      },
      "source": [
        "### spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "hidden": true,
        "id": "RtO_7v2LJGKc"
      },
      "source": [
        "[Training spaCy’s Statistical Models](https://spacy.io/usage/training)\n",
        "\n",
        "[NER](https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "-jmXlMnNSApm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(df_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Create train DataFrames for \"positive\" & \"negative\" sentiment data\n",
        "X_train_pos = X_train[X_train.sentiment == 'positive'].reset_index(drop=True)\n",
        "X_train_neg = X_train[X_train.sentiment == 'negative'].reset_index(drop=True)\n",
        "X_train_neu = X_train[X_train.sentiment == 'neutral'].reset_index(drop=True)\n",
        "df_train_pos = df_train[df_train.sentiment == 'positive'].reset_index(drop=True)\n",
        "df_train_neg = df_train[df_train.sentiment == 'negative'].reset_index(drop=True)\n",
        "df_train_neu = df_train[df_train.sentiment == 'neutral'].reset_index(drop=True)\n",
        "df_train_pos_neg = df_train[df_train.sentiment != 'neutral'].reset_index(drop=True)\n",
        "\n",
        "df_train.shape, df_test.shape, X_train.shape, X_test.shape, X_train_pos.shape, X_train_neg.shape, X_train_neu.shape, X_test.shape, df_train_pos.shape, df_train_neg.shape, df_train_neu.shape, df_train_pos_neg.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlrMndP-HM5x",
        "colab_type": "text"
      },
      "source": [
        "#### spaCy 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2IvLcUHL0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to format data that can be used for training a spaCy model \n",
        "def prep_spacy_data(df):\n",
        "  spacy_data = []\n",
        "  for index, row in df.iterrows():\n",
        "    selected_text = row.sel_text_clean\n",
        "    text = row.text_clean\n",
        "    start = text.find(selected_text)\n",
        "    end = start + len(selected_text)\n",
        "    spacy_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
        "  return spacy_data\n",
        "\n",
        "# Creating spaCy training data for \"positive\" & \"negative\" sentiments\n",
        "spacy_data_positive = prep_spacy_data(X_train_pos)\n",
        "spacy_data_negative = prep_spacy_data(X_train_neg)\n",
        "spacy_data_neutral = prep_spacy_data(X_train_neu)\n",
        "# spacy_data_positive = prep_spacy_data(df_train_pos)\n",
        "# spacy_data_negative = prep_spacy_data(df_train_neg)\n",
        "# spacy_data_neutral = prep_spacy_data(df_train_neu)\n",
        "\n",
        "len(spacy_data_positive), len(spacy_data_negative), len(spacy_data_neutral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RkYawaPl8Uo",
        "colab_type": "text"
      },
      "source": [
        "##### Model Pos Neg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU-gR5GNpiWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train spaCy model\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "import en_core_web_sm\n",
        "import en_core_web_lg\n",
        "\n",
        "def spacy_ner(model, train_data, output_dir, n_iter):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "      # nlp = spacy.load(model)  # load existing spaCy model\n",
        "      nlp = model.load()   # load existing spaCy model\n",
        "      print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "      nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "      print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "      ner = nlp.create_pipe(\"ner\")\n",
        "      nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "      ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "      for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in tqdm(range(n_iter)):\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            # batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "        print(\"Losses\", losses)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to extract \"selected_text\" - called by the extract_sel_text function\n",
        "def get_sel_text(text, model):\n",
        "    doc = model(text)\n",
        "    try:\n",
        "        sel_text = doc.ents[0].text\n",
        "    except:\n",
        "        sel_text = text\n",
        "    return (sel_text)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to setup extraction of \"selected_text\" - called by the spacy_pridict function\n",
        "def extract_sel_text(**kwargs):\n",
        "    models_pos = {}\n",
        "    models_neg = {}\n",
        "    sel_texts = []\n",
        "    \n",
        "    for key, value in kwargs.items():\n",
        "        if (re.findall(\"sentiment\", key)):\n",
        "            sentiment = value\n",
        "        elif (re.findall(\"text\", key)):\n",
        "            text = value\n",
        "        elif (re.findall(\"model.*pos\", key)):\n",
        "            models_pos[key] = value\n",
        "        elif (re.findall(\"model.*neg\", key)):\n",
        "            models_neg[key] = value\n",
        "        elif (re.findall(\"return_indx\", key)):\n",
        "            return_indx = value\n",
        "            \n",
        "    if (sentiment == 'positive'):\n",
        "        for name, model in models_pos.items():\n",
        "            sel_texts.append(get_sel_text(text, model))\n",
        "    else:\n",
        "        for name, model in models_neg.items():\n",
        "            sel_texts.append(get_sel_text(text, model))\n",
        "\n",
        "    sel_texts.sort(key=len)\n",
        "\n",
        "    return (sel_texts[return_indx])\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to predict \"selected_text\"\n",
        "def spacy_predict(model_ver, test_data, return_indx, lg=True, sm=True, none=True):\n",
        "  # Load models\n",
        "  try:\n",
        "    nlp_lg_pos = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_lg_pos_{model_ver}')\n",
        "    nlp_lg_neg = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_lg_neg_{model_ver}')\n",
        "  except:\n",
        "    lg = False\n",
        "  try:\n",
        "    nlp_sm_pos = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_sm_pos_{model_ver}')\n",
        "    nlp_sm_neg = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_sm_neg_{model_ver}')\n",
        "  except:\n",
        "    sm = False\n",
        "  try:\n",
        "    nlp_None_pos = spacy.load(f'{DATADIR}/models/spacy/model_None_pos_{model_ver}')\n",
        "    nlp_None_neg = spacy.load(f'{DATADIR}/models/spacy/model_None_neg_{model_ver}')\n",
        "  except:\n",
        "    none = False\n",
        "\n",
        "  df = test_data.copy(deep=True)     # Choose test dataset\n",
        "  sel_texts = []\n",
        "\n",
        "  for idx, row in tqdm(df.iterrows()):\n",
        "      if (row.sentiment != 'neutral'):\n",
        "        if ( (lg == True) and (sm == True) and (none == True) ):\n",
        "          sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                             model1_pos=nlp_lg_pos, model1_neg=nlp_lg_neg,\\\n",
        "                                             model2_pos=nlp_sm_pos, model2_neg=nlp_sm_neg,\\\n",
        "                                             model3_pos=nlp_None_pos, model3_neg=nlp_None_neg,\\\n",
        "                                             return_indx = return_indx))\n",
        "        elif ( (lg == True) and (sm == False) and (none == False) ):\n",
        "          sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                             model1_pos=nlp_lg_pos, model1_neg=nlp_lg_neg,\\\n",
        "                                             return_indx = return_indx))\n",
        "        elif ( (lg == False) and (sm == True) and (none == False) ):\n",
        "          sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                             model2_pos=nlp_sm_pos, model2_neg=nlp_sm_neg,\\\n",
        "                                             return_indx = return_indx))\n",
        "        elif ( (lg == False) and (sm == False) and (none == True) ):\n",
        "          sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                            model3_pos=nlp_None_pos, model3_neg=nlp_None_neg,\\\n",
        "                                            return_indx = return_indx))\n",
        "      else:\n",
        "          sel_texts.append(row.text_clean)\n",
        "\n",
        "  df['selected_text_pred'] = sel_texts\n",
        "  return (df)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to build spaCy model(s)\n",
        "def build_spacy_models(model_names, train_data, model_ver, n_iter):\n",
        "  for model_name, model in model_names.items():\n",
        "    for name, data in train_data.items():\n",
        "      output_dir = f'{DATADIR}/models/spacy/model_{model_name}_{name.split(\"_\")[-1]}_{model_ver}'\n",
        "      spacy_ner(model, data, output_dir, n_iter)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to compute Jaccard score \n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuJm_HRtmFFY",
        "colab_type": "text"
      },
      "source": [
        "##### Model pos, neg, neu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZcZf44EmJGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train spaCy model\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "import en_core_web_sm\n",
        "import en_core_web_lg\n",
        "\n",
        "def spacy_ner(model, train_data, output_dir, n_iter):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "      # nlp = spacy.load(model)  # load existing spaCy model\n",
        "      nlp = model.load()   # load existing spaCy model\n",
        "      print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "      nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "      print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "      ner = nlp.create_pipe(\"ner\")\n",
        "      nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "      ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "      for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in tqdm(range(n_iter)):\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            # batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "        print(\"Losses\", losses)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to extract \"selected_text\" - called by the extract_sel_text function\n",
        "def get_sel_text(text, model):\n",
        "    doc = model(text)\n",
        "    try:\n",
        "        sel_text = doc.ents[0].text\n",
        "    except:\n",
        "        sel_text = text\n",
        "    return (sel_text)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to setup extraction of \"selected_text\" - called by the spacy_pridict function\n",
        "def extract_sel_text(**kwargs):\n",
        "    models_pos = {}\n",
        "    models_neg = {}\n",
        "    models_neu = {}\n",
        "    sel_texts = []\n",
        "    \n",
        "    for key, value in kwargs.items():\n",
        "        if (re.findall(\"sentiment\", key)):\n",
        "            sentiment = value\n",
        "        elif (re.findall(\"text\", key)):\n",
        "            text = value\n",
        "        elif (re.findall(\"model.*pos\", key)):\n",
        "            models_pos[key] = value\n",
        "        elif (re.findall(\"model.*neg\", key)):\n",
        "            models_neg[key] = value\n",
        "        elif (re.findall(\"model.*neu\", key)):\n",
        "            models_neu[key] = value\n",
        "        elif (re.findall(\"return_indx\", key)):\n",
        "            return_indx = value\n",
        "            \n",
        "    if (sentiment == 'positive'):\n",
        "        for name, model in models_pos.items():\n",
        "            sel_texts.append(get_sel_text(text, model))\n",
        "    elif (sentiment == 'negative'):\n",
        "        for name, model in models_neg.items():\n",
        "            sel_texts.append(get_sel_text(text, model))\n",
        "    elif (sentiment == 'neutral'):\n",
        "        for name, model in models_neu.items():\n",
        "            sel_texts.append(get_sel_text(text, model))\n",
        "\n",
        "    sel_texts.sort(key=len)\n",
        "\n",
        "    return (sel_texts[return_indx])\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to predict \"selected_text\"\n",
        "def spacy_predict(model_ver, test_data, return_indx, lg=True, sm=True, none=True):\n",
        "  # Load models\n",
        "  try:\n",
        "    nlp_lg_pos = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_lg_pos_{model_ver}')\n",
        "    nlp_lg_neg = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_lg_neg_{model_ver}')\n",
        "    nlp_lg_neu = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_lg_neu_{model_ver}')\n",
        "  except:\n",
        "    lg = False\n",
        "  try:\n",
        "    nlp_sm_pos = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_sm_pos_{model_ver}')\n",
        "    nlp_sm_neg = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_sm_neg_{model_ver}')\n",
        "    nlp_sm_neu = spacy.load(f'{DATADIR}/models/spacy/model_en_core_web_sm_neu_{model_ver}')\n",
        "  except:\n",
        "    sm = False\n",
        "  try:\n",
        "    nlp_None_pos = spacy.load(f'{DATADIR}/models/spacy/model_None_pos_{model_ver}')\n",
        "    nlp_None_neg = spacy.load(f'{DATADIR}/models/spacy/model_None_neg_{model_ver}')\n",
        "    nlp_None_neu = spacy.load(f'{DATADIR}/models/spacy/model_None_neu_{model_ver}')\n",
        "  except:\n",
        "    none = False\n",
        "\n",
        "  df = test_data.copy(deep=True)     # Choose test dataset\n",
        "  sel_texts = []\n",
        "\n",
        "  for idx, row in tqdm(df.iterrows()):\n",
        "    if ( (lg == True) and (sm == True) and (none == True) ):\n",
        "      sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                        model1_pos=nlp_lg_pos, model1_neg=nlp_lg_neg, model1_neu=nlp_lg_neu,\\\n",
        "                                        model2_pos=nlp_sm_pos, model2_neg=nlp_sm_neg, model2_neu=nlp_sm_neu,\\\n",
        "                                        model3_pos=nlp_None_pos, model3_neg=nlp_None_neg, model3_neu=nlp_None_neu,\\\n",
        "                                          return_indx = return_indx))\n",
        "    elif ( (lg == True) and (sm == False) and (none == False) ):\n",
        "      sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                          model1_pos=nlp_lg_pos, model1_neg=nlp_lg_neg, model1_neu=nlp_lg_neu,\\\n",
        "                                          return_indx = return_indx))\n",
        "    elif ( (lg == False) and (sm == True) and (none == False) ):\n",
        "      sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                          model2_pos=nlp_sm_pos, model2_neg=nlp_sm_neg, model2_neu=nlp_sm_neu,\\\n",
        "                                          return_indx = return_indx))\n",
        "    elif ( (lg == False) and (sm == False) and (none == True) ):\n",
        "      sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "                                        model3_pos=nlp_None_pos, model3_neg=nlp_None_neg, model3_neu=nlp_None_neu,\\\n",
        "                                        return_indx = return_indx))\n",
        "\n",
        "  df['selected_text_pred'] = sel_texts\n",
        "  return (df)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to build spaCy model(s)\n",
        "def build_spacy_models(model_names, train_data, model_ver, n_iter):\n",
        "  for model_name, model in model_names.items():\n",
        "    for name, data in train_data.items():\n",
        "      output_dir = f'{DATADIR}/models/spacy/model_{model_name}_{name.split(\"_\")[-1]}_{model_ver}'\n",
        "      spacy_ner(model, data, output_dir, n_iter)\n",
        "#----------------------------------------------------------------------------------------#\n",
        "# Function to compute Jaccard score \n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK1chRh5mK0t",
        "colab_type": "text"
      },
      "source": [
        "##### Define parameters, models & datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRsFatQq5vsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define parameters, models, datasets\n",
        "train_data = {\"pos\" : spacy_data_positive, \"neg\" : spacy_data_negative, \"neu\" : spacy_data_neutral}\n",
        "test_data = X_test\n",
        "model_names = {\"None\" : None}\n",
        "# model_names = {\"en_core_web_sm\" : en_core_web_sm}\n",
        "# model_names = {\"en_core_web_lg\" : en_core_web_lg}\n",
        "# model_names = {\"None\" : None, \"en_core_web_sm\" : en_core_web_sm, \"en_core_web_lg\" : en_core_web_lg}\n",
        "n_iter = 15\n",
        "model_ver = 'v7'\n",
        "return_indx = 0   # length of selected_text prediction [0=>shortest, -1=>longest]\n",
        "\n",
        "# Launch model build, predict & score functions\n",
        "build_spacy_models(model_names, train_data, model_ver, n_iter)\n",
        "df = spacy_predict(model_ver, test_data, return_indx, lg=True, sm=True, none=True)\n",
        "\n",
        "jaccard_score = []\n",
        "for _, row in df.iterrows():\n",
        "    jaccard_score.append(jaccard(row.selected_text, row.selected_text_pred))\n",
        "df['jaccard_score'] = jaccard_score\n",
        "print (f'Overall Jaccard score = {np.mean(df.jaccard_score):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJO8U3K1NDO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define parameters, models, datasets\n",
        "train_data = {\"pos\" : spacy_data_positive, \"neg\" : spacy_data_negative, \"neu\" : spacy_data_neutral}\n",
        "test_data = X_test\n",
        "model_names = {\"None\" : None}\n",
        "# model_names = {\"en_core_web_sm\" : en_core_web_sm}\n",
        "# model_names = {\"en_core_web_lg\" : en_core_web_lg}\n",
        "# model_names = {\"None\" : None, \"en_core_web_sm\" : en_core_web_sm, \"en_core_web_lg\" : en_core_web_lg}\n",
        "n_iter = 15\n",
        "model_ver = 'v7'\n",
        "return_indx = 0   # length of selected_text prediction [0=>shortest, -1=>longest]\n",
        "\n",
        "# Launch model build, predict & score functions\n",
        "build_spacy_models(model_names, train_data, model_ver, n_iter)\n",
        "df = spacy_predict(model_ver, test_data, return_indx, lg=True, sm=True, none=True)\n",
        "\n",
        "jaccard_score = []\n",
        "for _, row in df.iterrows():\n",
        "    jaccard_score.append(jaccard(row.selected_text, row.selected_text_pred))\n",
        "df['jaccard_score'] = jaccard_score\n",
        "print (f'Overall Jaccard score = {np.mean(df.jaccard_score):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udrfxDPgYto7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_ = df[['textID', 'text', 'selected_text', 'selected_text_pred', 'jaccard_score']]\n",
        "df_.to_csv(f\"{DATADIR}/df.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KAzl_hQaSgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ = df[df.jaccard_score < 0.1]\n",
        "df_[df_.sentiment == 'negative'].to_csv(f\"{DATADIR}/df.csv\", sep='|', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvsZdzJzH6iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submission = df[['textID', 'selected_text']]\n",
        "df_submission.to_csv(\"./data/submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2al1QsaiHBDQ",
        "colab_type": "text"
      },
      "source": [
        "#### spaCy 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "ctABoqS4xEOz",
        "colab": {}
      },
      "source": [
        "# Function to format data that can be used for training a spaCy model \n",
        "def prep_spacy_data(df):\n",
        "  spacy_data = []\n",
        "  train_text = []\n",
        "  for _, row in df.iterrows():\n",
        "    selected_text = row.sel_text_clean\n",
        "    sentiment = row.sentiment\n",
        "    text = row.text_clean\n",
        "    start = text.find(selected_text)\n",
        "    end = start + len(selected_text)\n",
        "    spacy_data.append((text+\" :\"+sentiment, {\"entities\": [[start, end, 'selected_text']]}))\n",
        "  return spacy_data, train_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "3CaTLAJOYO6c",
        "colab": {}
      },
      "source": [
        "spacy_data, train_text = prep_spacy_data(df_train_pos_neg)\n",
        "train_text = []\n",
        "for _, row in df_train.iterrows():\n",
        "  if row.sentiment != 'neutral':\n",
        "    train_text.append(row.text_clean+\" :\"+row.sentiment)\n",
        "  else:\n",
        "    train_text.append(row.text_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "f009b3nBSAp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(spacy_data), df_train_pos_neg.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "3XLrs3umm_l8",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Train spaCy model\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "def main(model, train_data, output_dir, n_iter):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "      nlp = spacy.load(model)  # load existing spaCy model\n",
        "      print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "      nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "      print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "      ner = nlp.create_pipe(\"ner\")\n",
        "      nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "      ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "      for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly – but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # save model to output directory\n",
        "    if output_dir is not None:\n",
        "        output_dir = Path(output_dir)\n",
        "        if not output_dir.exists():\n",
        "            output_dir.mkdir()\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "data = spacy_data\n",
        "model_name = None\n",
        "# model_name = \"en_core_web_sm\"\n",
        "# model_name = \"en_core_web_lg\"\n",
        "n_iter = 30\n",
        "model_ver = 'v2_1'\n",
        "\n",
        "output_dir = f'{DATADIR}/models/spacy/model_{model_name}_{model_ver}'\n",
        "main(model_name, data, output_dir, n_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "hC86yOhTSAp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def get_sel_text(text, model):\n",
        "    doc = model(text)\n",
        "    try:\n",
        "        sel_text = doc.ents[0].text\n",
        "    except:\n",
        "        sel_text = text\n",
        "    return (sel_text)\n",
        "\n",
        "def extract_sel_text(text, models):\n",
        "    sel_texts = []\n",
        "    return_indx = -1   # length of selected_text prediction [0=>shortest, -1=>longest]\n",
        "    \n",
        "    for model in models:\n",
        "      sel_texts.append(get_sel_text(text, model))\n",
        "      \n",
        "    sel_texts.sort(key=len)\n",
        "\n",
        "    return (sel_texts[return_indx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "2wJA_UYVSAp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = 'v2_1'\n",
        "\n",
        "# Load models\n",
        "nlp_None = spacy.load(f'{DATADIR}/models/spacy/model_None_{version}')\n",
        "\n",
        "# df = X_test.copy(deep=True)     # Choose test dataset\n",
        "df = df_train.copy(deep=True)     # Choose test dataset\n",
        "sel_texts = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows()):\n",
        "    if (row.sentiment != 'neutral'):\n",
        "#         sel_texts.append(extract_sel_text(text=row.text_clean, sentiment=row.sentiment,\\\n",
        "#                                            model1_pos=nlp_lg_pos, model1_neg=nlp_lg_neg,\\\n",
        "#                                            model2_pos=nlp_sm_pos, model2_neg=nlp_sm_neg,\\\n",
        "#                                            model3_pos=nlp_None_pos, model3_neg=nlp_None_neg))\n",
        "        sel_texts.append(extract_sel_text(text=row.train_text, models=[nlp_None]))\n",
        "    else:\n",
        "        sel_texts.append(row.text_clean)\n",
        "\n",
        "df['selected_text_pred'] = sel_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrGgXKYuTITV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "o2Tj2wUCSAp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Jaccard score \n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "jaccard_score = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    jaccard_score.append(jaccard(row.selected_text, row.selected_text_pred))\n",
        "        \n",
        "df['jaccard_score'] = jaccard_score\n",
        "print (f'Overall Jaccard score = {np.mean(df.jaccard_score):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "tvt4X7rkSAp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "85-XQ8JWSAp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.countplot(x='text_wc', hue='sentiment', data=df[df.jaccard_score == 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "lI-bB_pNSAqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(df[df.sentiment == \"positive\"].jaccard_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "X5JCzvsrSAqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(df[df.sentiment == \"neutral\"].jaccard_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "oYbeq1ypSAqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submission = df[['textID', 'selected_text']]\n",
        "df_submission.to_csv(\"./data/submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "vKiAaiuJsB6E"
      },
      "source": [
        "### Azure ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "VEQPW6sUsEHX",
        "colab": {}
      },
      "source": [
        "name = 'NLPone618'\n",
        "endpoint = 'https://nlpone618.cognitiveservices.azure.com/'\n",
        "key = '5cef5507a7b94260adfc9170b8f1c321'\n",
        "key2 = 'fbd33fd5e562426091f1649634d9b0f5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Wpel9lNp2--M",
        "colab": {}
      },
      "source": [
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "def authenticate_client():\n",
        "    ta_credential = AzureKeyCredential(key)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=endpoint, credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "\n",
        "client = authenticate_client()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "kU9-q35g3dmj",
        "colab": {}
      },
      "source": [
        "def key_phrase_extraction_example(client):\n",
        "\n",
        "    try:\n",
        "        documents = list(df_train[df_train.sentiment != \"neutral\"].text[:5])\n",
        "        # documents = [\"My cat might need to see a veterinarian.\",\n",
        "        #              \"The quick brown fox jumps over a lazy dog.\"]\n",
        "        for i in range (len(documents)):\n",
        "          response = client.extract_key_phrases(documents = documents)[i]\n",
        "\n",
        "          if not response.is_error:\n",
        "              print(\"\\tKey Phrases:\")\n",
        "              for phrase in response.key_phrases:\n",
        "                  print(\"\\t\\t\", phrase)\n",
        "          else:\n",
        "              print(response.id, response.error)\n",
        "\n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "        \n",
        "key_phrase_extraction_example(client)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "Ae0Wkgdn3qxc",
        "colab": {}
      },
      "source": [
        "list(df_train[df_train.sentiment != \"neutral\"].text[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "hidden": true,
        "id": "q-I3s8225L1z",
        "colab": {}
      },
      "source": [
        "list(df_train[df_train.sentiment != \"neutral\"].selected_text[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOI-oFTHr3bt"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhioTzzBSAqu",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a\n",
        "\n",
        "https://www.kaggle.com/svadivazhagu/tf-bert-sentiment\n",
        "\n",
        "https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/\n",
        "\n",
        "https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=yDcqNlvVhL5W\n",
        "\n",
        "https://www.kaggle.com/mdmashurshalehin/tweet-sentiment-insight-eda\n",
        "\n",
        "https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
        "\n",
        "https://www.kaggle.com/koushiksahu/question-answering-roberta-for-absolute-beginners\n",
        "\n",
        "https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281\n",
        "\n",
        "https://www.curiousily.com/posts/intent-recognition-with-bert-using-keras-and-tensorflow-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp24zHMKOeK7",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare Train & Test data for Bert\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwvTVtzuL7LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create train DataFrames for \"positive\" & \"negative\" sentiment data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(df_train, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train_pos = X_train[X_train.sentiment == 'positive'].reset_index(drop=True)\n",
        "X_train_neg = X_train[X_train.sentiment == 'negative'].reset_index(drop=True)\n",
        "X_train_pos_neg = X_train[X_train.sentiment != 'neutral'].reset_index(drop=True)\n",
        "X_test_pos_neg = X_test[X_test.sentiment != 'neutral'].reset_index(drop=True)\n",
        "\n",
        "df_train_pos = df_train[df_train.sentiment == 'positive'].reset_index(drop=True)\n",
        "df_train_neg = df_train[df_train.sentiment == 'negative'].reset_index(drop=True)\n",
        "df_train_pos_neg = df_train[df_train.sentiment != 'neutral'].reset_index(drop=True)\n",
        "\n",
        "df_test_pos = df_test[df_test.sentiment == 'positive'].reset_index(drop=True)\n",
        "df_test_neg = df_test[df_test.sentiment == 'negative'].reset_index(drop=True)\n",
        "df_test_pos_neg = df_test[df_test.sentiment != 'neutral'].reset_index(drop=True)\n",
        "\n",
        "df_train.shape, df_train_pos.shape, df_train_neg.shape, df_train_pos_neg.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuZlXlO9nI1Y",
        "colab_type": "text"
      },
      "source": [
        "#### Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbHeMXGse-cv",
        "colab_type": "text"
      },
      "source": [
        "##### Tokenize using BertWordPieceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq3_ODnonkUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class prepDataBert():\n",
        "    def __init__(self, df, tokenizer, train=True, max_len=0):\n",
        "        self.text_ids, self.offsets = self.tokenize_data(df)\n",
        "        attention_masks = self.gen_attention_masks(self.text_ids)\n",
        "        self.token_type_ids = self.gen_token_type_ids(self.text_ids, max_len)\n",
        "        # self.max_len = max([len(text) for text in self.text_ids])\n",
        "        # print (self.max_len)\n",
        "        self.text_ids_padded = self.pad_data(self.text_ids, max_len)\n",
        "        self.attention_masks_padded = self.pad_data(attention_masks, max_len)\n",
        "        # self.token_type_ids_padded = self.pad_data(token_type_ids, max_len)\n",
        "        if (train):\n",
        "            self.targets_start, self.targets_end = self.get_targets(df, self.offsets)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to tokenize & get offsets using BertWordPieceTokenizer    \n",
        "    def tokenize_data(self, data):\n",
        "        text_ids = []\n",
        "        offsets = []\n",
        "        for _, row in data.iterrows():\n",
        "            s_tok = tokenizer.encode(row.sentiment).ids\n",
        "            t_tok = tokenizer.encode(row.text_clean).ids\n",
        "            text_ids.append(np.array(s_tok + t_tok[1:]))\n",
        "            offsets.append([(0, 0)] * 2 + tokenizer.encode(row.text_clean).offsets)\n",
        "        return (np.array(text_ids), offsets)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to create attention masks\n",
        "    def gen_attention_masks(self, text_ids):\n",
        "        attention_masks = []\n",
        "        # For each sentence...\n",
        "        for ids in text_ids:\n",
        "            # Create the attention mask.\n",
        "            #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "            #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "            att_mask = [int(token_id > 0) for token_id in ids]\n",
        "            # Store the attention mask for this sentence.\n",
        "            attention_masks.append(np.array(att_mask))\n",
        "        return(np.array(attention_masks))\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to create token type ids\n",
        "    def gen_token_type_ids(self, text_ids, max_len):\n",
        "        token_type_ids = []\n",
        "        # For each sentence...\n",
        "        for ids in text_ids:\n",
        "            # Set mask for sentiment (question) tokens to 1 and tweet (context) tokens to 0\n",
        "            tok_type_id = [0]*3 + [1]*(len(ids)-3) + [0]*(max_len-len(ids))\n",
        "            # Store the token type id for this sentence.\n",
        "            token_type_ids.append(np.array(tok_type_id))\n",
        "        return(np.array(token_type_ids))\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to pad data\n",
        "    def pad_data (self, data, max_len):\n",
        "        data_padded = pad_sequences(data, maxlen=max_len, dtype=\"long\", \n",
        "                                  value=0, truncating=\"post\", padding=\"post\")\n",
        "        return(data_padded)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to generate start & end targets based on text vs. selected_text matches\n",
        "    def get_targets(self, data, offsets):\n",
        "        targets_start = []\n",
        "        targets_end = []\n",
        "        i = 0\n",
        "        \n",
        "        # Find start & end index within tweet which matches selected_text\n",
        "        for _, row in data.iterrows():\n",
        "            tweet = row.text_clean\n",
        "            selected_text = row.sel_text_clean\n",
        "            len_st = len(selected_text)\n",
        "            idx0 = None\n",
        "            idx1 = None\n",
        "            for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
        "                if tweet[ind: ind+len_st] == selected_text:\n",
        "                    idx0 = ind\n",
        "                    idx1 = ind + len_st - 1\n",
        "                    break\n",
        "                    \n",
        "            # Mark portion of the tweet which matches selected_text with 1s\n",
        "            char_targets = [0] * len(tweet)\n",
        "            if idx0 != None and idx1 != None:\n",
        "                for ct in range(idx0, idx1 + 1):\n",
        "                    char_targets[ct] = 1\n",
        "\n",
        "            # targets_start = which index offset pair contains start of selected_text\n",
        "            # targets_end = which index offset pair contains end of selected_text\n",
        "            target_idx = []\n",
        "            for j, (offset1, offset2) in enumerate(offsets[i][3:]):\n",
        "                if sum(char_targets[offset1: offset2]) > 0:\n",
        "                    target_idx.append(j)\n",
        "            i += 1        \n",
        "            target_start = target_idx[0] + 3\n",
        "            target_end = target_idx[-1] + 3\n",
        "            targets_start.append(np.array(target_start))\n",
        "            targets_end.append(np.array(target_end))\n",
        "            \n",
        "        return (np.array(targets_start), np.array(targets_end))\n",
        "\n",
        "#------------------------------------------------------------------------------------------#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3N_LtYYnqUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenizers.BertWordPieceTokenizer(\n",
        "        f\"{DATADIR}/models/uncased_L-12_H-768_A-12/vocab.txt\", \n",
        "        lowercase=True\n",
        "    )\n",
        "\n",
        "x_train_pos_neg = prepDataBert(X_train_pos_neg, tokenizer, train=True, max_len=97)\n",
        "x_test_pos_neg = prepDataBert(X_test_pos_neg, tokenizer, train=True, max_len=97)\n",
        "\n",
        "train_pos_neg = prepDataBert(df_train_pos_neg, tokenizer, train=True, max_len=97)\n",
        "test_pos_neg = prepDataBert(df_test_pos_neg, tokenizer, train=False, max_len=97)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qByEV6vn7CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(15202,15205):\n",
        "    train_pos_neg.text_ids_padded[idx], train_pos_neg.offsets[idx], train_pos_neg.targets_start[idx], train_pos_neg.targets_end[idx]\n",
        "\n",
        "    s = train_pos_neg.offsets[idx][train_pos_neg.targets_start[idx]][0]\n",
        "    e = train_pos_neg.offsets[idx][train_pos_neg.targets_end[idx]][1]\n",
        "    print (df_train_pos_neg.text[idx])\n",
        "    print (df_train_pos_neg.selected_text[idx])\n",
        "    print (df_train_pos_neg.text_clean[idx][s:e])\n",
        "    print ('-------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaol2s-efHNG",
        "colab_type": "text"
      },
      "source": [
        "##### Create Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNRpJ5FbfLtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(max_seq_len, bert_model_dir, optimizer):\n",
        "    bert_params = bert.params_from_pretrained_ckpt(bert_model_dir)\n",
        "    bert_model = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n",
        "    att = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n",
        "    tok_type_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n",
        "\n",
        "    x = bert_model([input_ids, att])\n",
        "    # x = bert_model([input_ids, tok_type_ids])\n",
        "\n",
        "    #*******************Mod 1*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x)\n",
        "    # x1 = tf.keras.layers.Conv1D(1,1)(x1) \n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x) \n",
        "    # x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    #*******************Mod 2*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x)\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 3*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x)\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 4*******************\n",
        "    x1 =  tf.keras.layers.Dropout(0.1)(x)\n",
        "    x1 =  tf.keras.layers.GRU(1024, return_sequences=True)(x1)\n",
        "    x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    x2 =  tf.keras.layers.Dropout(0.1)(x)\n",
        "    x2 =  tf.keras.layers.GRU(1024, return_sequences=True)(x2)\n",
        "    x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 5*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x)\n",
        "    # x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x) \n",
        "    # x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************************************\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[input_ids, att], outputs=[x1,x2])\n",
        "    # model = tf.keras.Model(inputs=[input_ids, tok_type_ids], outputs=[x1,x2])\n",
        "    \n",
        "    model.build(input_shape=(None, max_seq_len))\n",
        "    load_stock_weights(bert_model, f'{bert_model_dir}/bert_model.ckpt')\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return (model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbkmar-fT7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model_dir = f'{DATADIR}/models/uncased_L-12_H-768_A-12'\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=3e-5,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model = create_model(97, bert_model_dir, optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSRQNx-EfW80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'bert9-model'\n",
        "if not os.path.exists(f'{DATADIR}/models/{model_name}'):\n",
        "    os.makedirs(f'{DATADIR}/models/{model_name}')\n",
        "\n",
        "model.save(f'{DATADIR}/models/{model_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfeTTODs1g9i",
        "colab_type": "text"
      },
      "source": [
        "#### Roberta, XLM-Roberta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9i0xyvbjbSe",
        "colab_type": "text"
      },
      "source": [
        "##### Tokenize using RobertaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvQ87jrCCp0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class prepDataRoberta():\n",
        "    def __init__(self, df, tokenizer, train=True, max_len=0):\n",
        "        self.text_ids, attention_masks, self.offsets = self.tokenize_data(df)\n",
        "        # self.max_len = max([len(text) for text in self.text_ids])\n",
        "        # print (self.max_len)\n",
        "        self.text_ids_padded = self.pad_data(self.text_ids, max_len)\n",
        "        self.attention_masks_padded = self.pad_data(attention_masks, max_len)\n",
        "        self.token_type_ids_padded = np.zeros((df.shape[0],max_len), dtype=np.int32)\n",
        "        if (train):\n",
        "            self.targets_start, self.targets_end = self.get_targets(df, self.offsets)\n",
        "    #------------------------------------------------------------------------------------------#\n",
        "    # Method to tokenize & get attention masks, offsets using RobertaTokenizer Tokenizer\n",
        "    def tokenize_data(self, data):\n",
        "        text_ids = []\n",
        "        attention_masks = []\n",
        "        offsets = []\n",
        "        for _, row in data.iterrows():\n",
        "          tok = tokenizer.encode_plus(row.sentiment, row.text_clean)\n",
        "          tok_ids = tok['input_ids']\n",
        "          att_masks = tok['attention_mask']\n",
        "          text_ids.append(np.array(tok_ids))\n",
        "          attention_masks.append(np.array(att_masks))\n",
        "          s = 0\n",
        "          offset = []\n",
        "          for i in tok_ids[4:-1]:\n",
        "            e = s + len(tokenizer.decode(i))\n",
        "            # print ((s,e))\n",
        "            offset.append((s,e))\n",
        "            s = e\n",
        "          offset_ = 4*[(0,0)] + offset + [(0,0)]\n",
        "          offsets.append(offset_)\n",
        "        return(text_ids, attention_masks, offsets)\n",
        "    #------------------------------------------------------------------------------------------#\n",
        "    # Method to pad data\n",
        "    def pad_data (self, data, max_len):\n",
        "        data_padded = pad_sequences(data, maxlen=max_len, dtype=\"long\", \n",
        "                                  value=1, truncating=\"post\", padding=\"post\")\n",
        "        return(data_padded)\n",
        "    #------------------------------------------------------------------------------------------#\n",
        "    # Method to generate start & end targets based on text vs. selected_text matches\n",
        "    def get_targets(self, data, offsets):\n",
        "        targets_start = []\n",
        "        targets_end = []\n",
        "        i = 0\n",
        "        \n",
        "        # Find start & end index within tweet which matches selected_text\n",
        "        for _, row in data.iterrows():\n",
        "            tweet = row.text_clean\n",
        "            selected_text = row.sel_text_clean\n",
        "            len_st = len(selected_text)\n",
        "            idx0 = None\n",
        "            idx1 = None\n",
        "            for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
        "                if tweet[ind: ind+len_st] == selected_text:\n",
        "                    idx0 = ind\n",
        "                    idx1 = ind + len_st - 1\n",
        "                    break\n",
        "                    \n",
        "            # Mark portion of the tweet which matches selected_text with 1s\n",
        "            char_targets = [0] * len(tweet)\n",
        "            if idx0 != None and idx1 != None:\n",
        "                for ct in range(idx0, idx1 + 1):\n",
        "                    char_targets[ct] = 1\n",
        "\n",
        "            # targets_start = which index offset pair contains start of selected_text\n",
        "            # targets_end = which index offset pair contains end of selected_text\n",
        "            target_idx = []\n",
        "            for j, (offset1, offset2) in enumerate(offsets[i][4:]):\n",
        "                if sum(char_targets[offset1: offset2]) > 0:\n",
        "                    target_idx.append(j)\n",
        "            i += 1        \n",
        "            target_start = target_idx[0] + 4\n",
        "            target_end = target_idx[-1] + 4\n",
        "            targets_start.append(np.array(target_start))\n",
        "            targets_end.append(np.array(target_end))\n",
        "            \n",
        "        return (np.array(targets_start), np.array(targets_end))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb_DwukXlXCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "tokenizer = transformers.RobertaTokenizer.from_pretrained(f\"{DATADIR}/models/roberta-base\")\n",
        "\n",
        "x_train_pos_neg = prepDataRoberta(X_train_pos_neg, tokenizer, train=True, max_len=75)\n",
        "x_test_pos_neg = prepDataRoberta(X_test_pos_neg, tokenizer, train=True, max_len=75)\n",
        "train_pos_neg = prepDataRoberta(df_train_pos_neg, tokenizer, train=True, max_len=75)\n",
        "test_pos_neg = prepDataRoberta(df_test_pos_neg, tokenizer, train=False, max_len=75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFL5KQESO7tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(15202,15205):\n",
        "    train_pos_neg.text_ids_padded[idx], train_pos_neg.offsets[idx], train_pos_neg.targets_start[idx], train_pos_neg.targets_end[idx]\n",
        "\n",
        "    s = train_pos_neg.offsets[idx][train_pos_neg.targets_start[idx]][0]\n",
        "    e = train_pos_neg.offsets[idx][train_pos_neg.targets_end[idx]][1]\n",
        "    print (df_train_pos_neg.text[idx])\n",
        "    print (df_train_pos_neg.selected_text[idx])\n",
        "    print (re.sub(r'^\\s','',df_train_pos_neg.text_clean[idx][s:e]))\n",
        "    print ('-------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGPceG93fhIC",
        "colab_type": "text"
      },
      "source": [
        "##### Tokenize using SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMHUCHc-703",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class prepDataXLMRoberta():\n",
        "    def __init__(self, df, sp, spt, train=True, max_len=0):\n",
        "        self.text_ids, self.offsets = self.tokenize_data(df)\n",
        "        attention_masks = self.gen_attention_masks(self.text_ids)\n",
        "        # self.max_len = max([len(text) for text in self.text_ids])\n",
        "        # print (self.max_len)\n",
        "        self.text_ids_padded = self.pad_data(self.text_ids, max_len)\n",
        "        self.attention_masks_padded = self.pad_data(attention_masks, max_len)\n",
        "        self.token_type_ids_padded = np.zeros((df.shape[0],max_len), dtype=np.int32)\n",
        "        if (train):\n",
        "            self.targets_start, self.targets_end = self.get_targets(df, self.offsets)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to tokenize & get offsets\n",
        "    def get_spt_pieces_data(self, spt):\n",
        "        ids = []\n",
        "        offsets = []\n",
        "        for piece in spt.pieces:\n",
        "            ids.append(piece.id)\n",
        "            offsets.append((piece.begin, piece.end))\n",
        "        ids = [1] + ids + [2]\n",
        "        offsets = [(0,0)] + offsets + [(0,0)]\n",
        "        return (ids, offsets)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to tokenize & get offsets using SentencePiece Tokenizer\n",
        "    def tokenize_data(self, data):\n",
        "        text_ids = []\n",
        "        offsets = []\n",
        "        for _, row in data.iterrows():\n",
        "            spt.ParseFromString(sp.encode_as_serialized_proto(row.sentiment))\n",
        "            (sentiment_ids, sentiment_offsets) = self.get_spt_pieces_data(spt)\n",
        "            spt.ParseFromString(sp.encode_as_serialized_proto(row.text))\n",
        "            (token_ids, token_offsets) = self.get_spt_pieces_data(spt)\n",
        "            text_ids.append(np.array(sentiment_ids + token_ids))\n",
        "            offsets.append([(0, 0)] * 3 + token_offsets)\n",
        "        return (np.array(text_ids), offsets)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to create attention masks\n",
        "    def gen_attention_masks(self, text_ids):\n",
        "        attention_masks = []\n",
        "        # For each sentence...\n",
        "        for ids in text_ids:\n",
        "            # Create the attention mask.\n",
        "            #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "            #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "            att_mask = [int(token_id > 0) for token_id in ids]\n",
        "            # Store the attention mask for this sentence.\n",
        "            attention_masks.append(np.array(att_mask))\n",
        "        return(np.array(attention_masks))\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to create token type ids\n",
        "    def gen_token_type_ids(self, text_ids, max_len):\n",
        "        token_type_ids = []\n",
        "        # For each sentence...\n",
        "        for ids in text_ids:\n",
        "            # Set mask for sentiment (question) tokens to 1 and tweet (context) tokens to 0\n",
        "            tok_type_id = [0]*4 + [1]*(len(ids)-4) + [0]*(max_len-len(ids))\n",
        "            # Store the token type id for this sentence.\n",
        "            token_type_ids.append(np.array(tok_type_id))\n",
        "        return(np.array(token_type_ids))\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to pad data\n",
        "    def pad_data (self, data, max_len):\n",
        "        data_padded = pad_sequences(data, maxlen=max_len, dtype=\"long\", \n",
        "                                  value=1, truncating=\"post\", padding=\"post\")\n",
        "        return(data_padded)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to generate start & end targets based on text vs. selected_text matches\n",
        "    def get_targets(self, data, offsets):\n",
        "        targets_start = []\n",
        "        targets_end = []\n",
        "        i = 0\n",
        "        \n",
        "        # Find start & end index within tweet which matches selected_text\n",
        "        for _, row in data.iterrows():\n",
        "            tweet = row.text\n",
        "            selected_text = row.selected_text\n",
        "            len_st = len(selected_text)\n",
        "            idx0 = None\n",
        "            idx1 = None\n",
        "            for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
        "                if tweet[ind: ind+len_st] == selected_text:\n",
        "                    idx0 = ind\n",
        "                    idx1 = ind + len_st - 1\n",
        "                    break\n",
        "                    \n",
        "            # Mark portion of the tweet which matches selected_text with 1s\n",
        "            char_targets = [0] * len(tweet)\n",
        "            if idx0 != None and idx1 != None:\n",
        "                for ct in range(idx0, idx1 + 1):\n",
        "                    char_targets[ct] = 1\n",
        "\n",
        "            # targets_start = which index offset pair contains start of selected_text\n",
        "            # targets_end = which index offset pair contains end of selected_text\n",
        "            target_idx = []\n",
        "            for j, (offset1, offset2) in enumerate(offsets[i][4:]):\n",
        "                if sum(char_targets[offset1: offset2]) > 0:\n",
        "                    target_idx.append(j)\n",
        "            i += 1        \n",
        "            target_start = target_idx[0] + 4\n",
        "            target_end = target_idx[-1] + 4\n",
        "            targets_start.append(np.array(target_start))\n",
        "            targets_end.append(np.array(target_end))\n",
        "            \n",
        "        return (np.array(targets_start), np.array(targets_end))\n",
        "\n",
        "#------------------------------------------------------------------------------------------#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv9tNSy-QLAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, f\"{DATADIR}/models/sentencepiece/\")\n",
        "import sentencepiece as spm\n",
        "import sentencepiece_pb2\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{DATADIR}/models/tf-xlm-roberta-base/sentencepiece.bpe.model\")\n",
        "spt = sentencepiece_pb2.SentencePieceText()\n",
        "\n",
        "x_train_pos_neg = prepDataXLMRoberta(X_train_pos_neg, sp, spt, train=True, max_len=78)\n",
        "x_test_pos_neg = prepDataXLMRoberta(X_test_pos_neg, sp, spt, train=True, max_len=78)\n",
        "train_pos_neg = prepDataXLMRoberta(df_train_pos_neg, sp, spt, train=True, max_len=78)\n",
        "test_pos_neg = prepDataXLMRoberta(df_test_pos_neg, sp, spt, train=False, max_len=78)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1jk2UsqRgTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(15202,15205):\n",
        "    train_pos_neg.text_ids_padded[idx], train_pos_neg.offsets[idx], train_pos_neg.targets_start[idx], train_pos_neg.targets_end[idx]\n",
        "\n",
        "    s = train_pos_neg.offsets[idx][train_pos_neg.targets_start[idx]][0]\n",
        "    e = train_pos_neg.offsets[idx][train_pos_neg.targets_end[idx]][1]\n",
        "    print (df_train_pos_neg.text[idx])\n",
        "    print (df_train_pos_neg.selected_text[idx])\n",
        "    print (df_train_pos_neg.text[idx][s:e])\n",
        "    print ('-------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNaxmvSfpvD",
        "colab_type": "text"
      },
      "source": [
        "##### Create Roberta Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXwLJxNpfyv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_roberta_model(max_len, roberta_model_dir, optimizer):\n",
        "    \n",
        "    text_ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    att_masks = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    tok_type_ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    \n",
        "    \n",
        "    config = RobertaConfig.from_pretrained(f'{roberta_model_dir}/config.json')\n",
        "    roberta_model = TFRobertaModel.from_pretrained(f'{roberta_model_dir}/tf_model.h5',config=config)\n",
        "    # roberta_model = TFRobertaModel.from_pretrained(roberta_model_dir)\n",
        "\n",
        "    x = roberta_model(text_ids, attention_mask=att_masks)\n",
        "    # x = roberta_model(text_ids,attention_mask=att_masks,token_type_ids=tok_type_ids)\n",
        "    \n",
        "    #*******************Mod 1*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    #*******************Mod 2*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    #*******************Mod 3*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 4*******************\n",
        "    # x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x1)\n",
        "    # x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    # x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x2 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x2)\n",
        "    # x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    # x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    # x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 5*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 6*******************\n",
        "    # x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768, return_sequences=True))(x1)\n",
        "    # x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    # x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x2 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768, return_sequences=True))(x2)\n",
        "    # x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    # x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    # x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 7*******************\n",
        "    x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 =  tf.keras.layers.GRU(1024, return_sequences=True)(x1)\n",
        "    x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x2 =  tf.keras.layers.GRU(1024, return_sequences=True)(x2)\n",
        "    x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************************************\n",
        "    model = tf.keras.models.Model(inputs=[text_ids, att_masks], outputs=[x1,x2])\n",
        "    # model = tf.keras.models.Model(inputs=[text_ids, att_masks, tok_type_ids], outputs=[x1,x2])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return (model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x69GbEtifzdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_model_dir = f'{DATADIR}/models/roberta-base/'\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=3e-5,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0001,\n",
        "    power=1.0,\n",
        "    cycle=False,\n",
        "    )\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "model = create_roberta_model(75, roberta_model_dir, optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxWHRcgBgXnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'r1-model'\n",
        "# model.save(f'{DATADIR}/models/{model_name}')\n",
        "model.save_weights(f'{DATADIR}/models/{model_name}/{model_name}.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg5523UTHL0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del(model)\n",
        "# model = tf.keras.models.load_model(f'{DATADIR}/models/r1-model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ychFbtejfvWc",
        "colab_type": "text"
      },
      "source": [
        "##### Create XLM-Roberta Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA5P701Kf0TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_xlm_roberta_model(max_len, xlm_roberta_model_dir, optimizer):\n",
        "    \n",
        "    text_ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    att_masks = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    tok_type_ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
        "    \n",
        "    config = XLMRobertaConfig.from_pretrained(f'{xlm_roberta_model_dir}/config.json')\n",
        "    xlm_roberta_model = TFXLMRobertaModel.from_pretrained(f'{xlm_roberta_model_dir}/tf_model.h5',config=config)\n",
        "\n",
        "    x = xlm_roberta_model(text_ids,attention_mask=att_masks)\n",
        "    # x = xlm_roberta_model(text_ids,attention_mask=att_masks,token_type_ids=tok_type_ids)\n",
        "    \n",
        "    #*******************Mod 1*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    #*******************Mod 2*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    #*******************Mod 3*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x1)\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(128, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(64, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Conv1D(32, 1, activation='relu')(x2)\n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 4*******************\n",
        "    # x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x1)\n",
        "    # x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    # x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x2 =  tf.keras.layers.LSTM(1024, return_sequences=True)(x2)\n",
        "    # x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    # x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    # x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 5*******************\n",
        "    # x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    # x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 = tf.keras.layers.Flatten()(x1)\n",
        "    # x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    # x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    # x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    # x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    # x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 = tf.keras.layers.Flatten()(x2)\n",
        "    # x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 6*******************\n",
        "    # x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x1 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768, return_sequences=True))(x1)\n",
        "    # x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    # x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    # x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    # x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    # x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    # x2 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768, return_sequences=True))(x2)\n",
        "    # x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    # x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    # x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    # x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************Mod 7*******************\n",
        "    x1 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 =  tf.keras.layers.GRU(1024, return_sequences=True)(x1)\n",
        "    x1 =  tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 =  tf.keras.layers.Dense(1)(x1)\n",
        "    x1 =  tf.keras.layers.Flatten()(x1)\n",
        "    x1 =  tf.keras.layers.Activation('softmax')(x1)\n",
        "\n",
        "    x2 =  tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x2 =  tf.keras.layers.GRU(1024, return_sequences=True)(x2)\n",
        "    x2 =  tf.keras.layers.LeakyReLU()(x2)    \n",
        "    x2 =  tf.keras.layers.Dense(1)(x2)\n",
        "    x2 =  tf.keras.layers.Flatten()(x2)\n",
        "    x2 =  tf.keras.layers.Activation('softmax')(x2)\n",
        "    #*******************************************\n",
        "    model = tf.keras.models.Model(inputs=[text_ids, att_masks], outputs=[x1,x2])\n",
        "    # model = tf.keras.models.Model(inputs=[text_ids, att_masks, tok_type_ids], outputs=[x1,x2])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return (model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9oPoEef0FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xlm_roberta_model_dir = f'{DATADIR}/models/tf-xlm-roberta-base/'\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=3e-5,\n",
        "    decay_steps=10000,\n",
        "    end_learning_rate=0.0001,\n",
        "    power=1.0,\n",
        "    cycle=False,\n",
        "    )\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "model = create_xlm_roberta_model(75, xlm_roberta_model_dir, optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWALTSeLgODz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'r1-model'\n",
        "model.save(f'{DATADIR}/models/{model_name}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSCiXPHAKGIG",
        "colab_type": "text"
      },
      "source": [
        "#### Albert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_B4vQOygSNu",
        "colab_type": "text"
      },
      "source": [
        "##### Tokenize using AlbertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IdteAy4Kqba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class prepDataBert():\n",
        "    def __init__(self, df, tokenizer, train=True, max_len=0):\n",
        "        self.max_len = max_len\n",
        "        self.text_ids, attention_masks = self.tokenize_data(df.text_clean)\n",
        "        #self.max_len = max([len(text) for text in self.text_ids])\n",
        "        self.text_ids_padded = self.pad_data(self.text_ids, self.max_len)\n",
        "        self.attention_masks_padded = self.pad_data(attention_masks, self.max_len)\n",
        "        if (train):\n",
        "            self.selected_text_ids, _ = self.tokenize_data(df.sel_text_clean)\n",
        "            self.targets_start, self.targets_end, self.textIDs = self.get_targets(\\\n",
        "                                                                                  self.text_ids,\\\n",
        "                                                                                  self.selected_text_ids,\\\n",
        "                                                                                  df.textID,\\\n",
        "                                                                                  self.max_len)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to tokenize data using AlbertTokenizer    \n",
        "    def tokenize_data(self, data):\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        for sent in data:\n",
        "            t = tokenizer.encode_plus(sent)\n",
        "            input_ids.append(t['input_ids'])\n",
        "            attention_masks.append(t['attention_mask'])\n",
        "        return (np.array(input_ids), np.array(attention_masks))\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to pad data\n",
        "    def pad_data (self, data, max_len):\n",
        "        data_padded = pad_sequences(data, maxlen=max_len, dtype=\"long\", \n",
        "                                  value=0, truncating=\"post\", padding=\"post\")\n",
        "        return(data_padded)\n",
        "#------------------------------------------------------------------------------------------#\n",
        "    # Method to generate targets based on text vs. selected_text matches\n",
        "    def get_targets(self, data1, data2, data3, max_len):\n",
        "        self.targets_start = []\n",
        "        self.targets_end = []\n",
        "        self.textIDs = []\n",
        "        for text_ids, sel_text_ids, textID in zip(data1, data2, data3):\n",
        "            a = text_ids\n",
        "            b = sel_text_ids[1:-1]\n",
        "            bool_a = np.in1d(a,b)\n",
        "            matched_indices = np.where(bool_a == True)[0]\n",
        "            target_start = np.zeros((max_len,), dtype=int)\n",
        "            target_end = np.zeros((max_len,), dtype=int)\n",
        "            try:\n",
        "                target_start[matched_indices[0]] = 1\n",
        "                target_end[matched_indices[len(b)-1]] = 1\n",
        "            except:\n",
        "                self.textIDs.append(textID)\n",
        "                target_end[matched_indices[-1]] = 1\n",
        "            self.targets_start.append(np.array(target_start))\n",
        "            self.targets_end.append(np.array(target_end))\n",
        "        return(np.array(self.targets_start), np.array(self.targets_end), self.textIDs)\n",
        "        \n",
        "#------------------------------------------------------------------------------------------#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuEvqq_tKI2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(f'{DATADIR}/models/albert-large-v1/')\n",
        "\n",
        "train_pos = prepDataBert(df_train_pos, tokenizer, train=True, max_len=95)\n",
        "train_neg = prepDataBert(df_train_neg, tokenizer, train=True, max_len=95)\n",
        "test_pos = prepDataBert(df_test_pos, tokenizer, train=False, max_len=95)\n",
        "test_neg = prepDataBert(df_test_neg, tokenizer, train=False, max_len=95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtFNnu-rosj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "at = AlbertTokenizer.from_pretrained(f'{DATADIR}/models/albert-large-v1/')\n",
        "at.encode_plus(['This is a test'], ['This is a a test also'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMYNr_RDsEvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "at.decode([3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnBAzxnoOtU0",
        "colab_type": "text"
      },
      "source": [
        "#### Train & chkpt model on data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bi3xrBRPqj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FitModelCheckpoint(checkpoint_path, data, epochs): \n",
        "  # Create a callback that saves the model's weights\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                   save_weights_only=True,\n",
        "                                                   verbose=1)\n",
        "  \n",
        "  # Train the model with the new callback\n",
        "  history = model.fit([data.text_ids_padded, data.attention_masks_padded], \n",
        "            [data.targets_start, data.targets_end],\n",
        "            epochs=epochs,\n",
        "            validation_split=0.2,\n",
        "            batch_size=16,\n",
        "            shuffle=True,\n",
        "            callbacks=[cp_callback])  # Pass callback to training\n",
        "\n",
        "  # history = model.fit([data.text_ids_padded, data.attention_masks_padded, data.token_type_ids_padded], \n",
        "  #           [data.targets_start, data.targets_end],\n",
        "  #           epochs=epochs,\n",
        "  #           validation_split=0.2,\n",
        "  #           batch_size=16,\n",
        "  #           shuffle=True,\n",
        "  #           callbacks=[cp_callback])  # Pass callback to training\n",
        "\n",
        "  return (history)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpilmPTFULKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = x_train_pos_neg\n",
        "# data = train_pos_neg\n",
        "epochs = 3\n",
        "\n",
        "# checkpoint_dir = f'{DATADIR}/models/bert9-training'\n",
        "checkpoint_dir = f'{DATADIR}/models/r4-1-training'\n",
        "if not os.path.exists(f'{checkpoint_dir}'):\n",
        "    os.makedirs(f'{checkpoint_dir}')\n",
        "\n",
        "checkpoint_path = f'{checkpoint_dir}/cp.ckpt'\n",
        "print (checkpoint_path)\n",
        "\n",
        "history = FitModelCheckpoint(checkpoint_path, data, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qla67XDgjOhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r2-1-training/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoalOugmY4Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_hist(history):\n",
        "  history_dict=history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values=history_dict['val_loss']\n",
        "  # plt.figure(figsize=(10,6))\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.plot(loss_values, color='Blue', linestyle='dashed', marker='o', label='Training Loss')\n",
        "  plt.plot(val_loss_values,color='Red', label=\n",
        "           'Validation Loss')\n",
        "  plt.legend()\n",
        "  # plt.show()\n",
        "\n",
        "plot_hist(history)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt_0cZ66nd2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !tar -zcvf \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/roberta-base.tar.gz\" \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/roberta-base\"\n",
        "\n",
        "# !tar -zcvf \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/bert9-model.tar.gz\" \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/bert9-model\"\n",
        "# !tar -zcvf \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/bert9-training.tar.gz\" \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/bert9-training\"\n",
        "\n",
        "# !tar -zcvf \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r1-model.tar.gz\" \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r1-model\"\n",
        "!tar -zcvf \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r3-1-training.tar.gz\" \"/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r3-1-training\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceP-y4D2twLj",
        "colab_type": "text"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-da1qhvjrOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_pos = tf.keras.models.load_model(f'{DATADIR}/models/bert4-model')\n",
        "# model_neg = tf.keras.models.load_model(f'{DATADIR}/models/bert4-model')\n",
        "# model_pos.load_weights(f'{DATADIR}/models/bert4-training-pos/cp.ckpt')\n",
        "# model_neg.load_weights(f'{DATADIR}/models/bert4-training-neg/cp.ckpt')\n",
        "\n",
        "\n",
        "# model = tf.keras.models.load_model(f'{DATADIR}/models/bert5-model')\n",
        "# model.load_weights(f'{DATADIR}/models/bert5-training/cp.ckpt')\n",
        "\n",
        "# model = tf.keras.models.load_model(f'{DATADIR}/models/r1-model')\n",
        "model.load_weights(f'/content/gdrive/My Drive/Colab Notebooks/TweetSentiment/data/models/r2-1-training/cp.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuyaufYJxIU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_text = []\n",
        "bad_preds = []\n",
        "i = 0\n",
        "# df = df_test\n",
        "# tok_data = test_pos_neg\n",
        "# df = df_train\n",
        "# tok_data = train_pos_neg\n",
        "df = X_test\n",
        "tok_data = x_test_pos_neg\n",
        "roberta = True\n",
        "\n",
        "preds = model.predict([tok_data.text_ids_padded, tok_data.attention_masks_padded])\n",
        "# preds = model.predict([tok_data.text_ids_padded, tok_data.attention_masks_padded, tok_data.token_type_ids_padded])\n",
        "\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "  if (row.sentiment != 'neutral'):\n",
        "    flag = True\n",
        "    target_start = preds[0][i].argmax()\n",
        "    target_end = preds[1][i].argmax()\n",
        "    try:\n",
        "      start = tok_data.offsets[i][target_start][0]\n",
        "      end = tok_data.offsets[i][target_end][1]\n",
        "    except:\n",
        "      bad_preds.append(i)\n",
        "      flag = False\n",
        "    if (flag):\n",
        "      if (roberta):\n",
        "        selected_text.append(re.sub(r'^\\s','',row.text_clean[start:end]))\n",
        "      else:\n",
        "        selected_text.append(row.text_clean[start:end])\n",
        "    else:\n",
        "      selected_text.append(row.text_clean)  \n",
        "    i += 1\n",
        "  else:\n",
        "    selected_text.append(row.text_clean)\n",
        "    \n",
        "df['selected_text_pred'] = selected_text\n",
        "print (f'Bad predictions = {len(bad_preds)}')\n",
        "\n",
        "# Function to compute Jaccard score \n",
        "def jaccard(str1, str2): \n",
        "      a = set(str1.lower().split()) \n",
        "      b = set(str2.lower().split())\n",
        "      c = a.intersection(b)\n",
        "      try:\n",
        "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "      except:\n",
        "        return (0)\n",
        "\n",
        "if not (df.equals(df_test)):\n",
        "  jaccard_score = []\n",
        "  for _, row in df.iterrows():\n",
        "      jaccard_score.append(jaccard(row.selected_text, row.selected_text_pred))\n",
        "  df['jaccard_score'] = jaccard_score\n",
        "  print (f'Overall Jaccard score = {np.mean(df.jaccard_score):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k78O80XPzBjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_submission = df[['textID', 'selected_text']]\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "df_submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u41zjZ7JVIZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}